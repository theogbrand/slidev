---
# You can also start simply with 'default'
theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: "Multimodal Reasoning & RLHF: Training a Process Reward Model"
info: |
  A presentation on training a Process Reward Model using an agentic approach
  to verify the validity of reasoning generated by a Vision Language Model.

  Learn more at [Sli.dev](https://sli.dev)
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true
# open graph
# seoMeta:
#  ogImage: https://cover.sli.dev
# add scripts for Twitter widget
headersFrontmatter:
  scripts:
    - src: https://platform.twitter.com/widgets.js
      async: true
---

# Multimodal Reasoning & RL

### "Age of Experience", Test-time Scaling, Self-Improving Agents, PRMs

<div class="abs-br m-6 text-xl">
  <button @click="$slidev.nav.openInEditor()" title="Open in Editor" class="slidev-icon-btn">
    <carbon:edit />
  </button>
  <!-- <a href="https://github.com/slidevjs/slidev" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a> -->
</div>

<!--
The last comment block of each slide will be treated as slide notes. It will be visible and editable in Presenter Mode along with the slide. [Read more in the docs](https://sli.dev/guide/syntax.html#notes)
-->

---
layout: default
---

# Outline

1.  **Background: The Rise of Multimodal Reasoning**
    - Frontier AI products (Operator, Manus)
    - Enterprise Applications
2.  **The Age of Experience: Self-Improving Agents with RL and PRMs**
    - Improving agents with experience without human annotation
    - Agentic Verification
3.  **Core Mechanism 1: Monte Carlo Rollouts**
    - Generating experiences
4.  **Core Mechanism 2: Agentic Verification of Reasoning Traces**
    - Verifying reasoning traces using specialized agents with tools
5.  **RL Policy Model, Inference with PRM**
    - Use the newly trained PRM to improve model inference at test time

---
layout: default
---

# Background: The Rise of Multimodal Reasoning

## Industry Emergence

- **Multimodal reasoning** powers cutting-edge products like:
  - OpenAI Operator
  - Manus
  - ChatGPT's o3 agentic capabilities
- These systems can zoom into images, search the web, and use code interpreters to answer complex multimodal queries
- Represents a significant leap beyond unimodal AI systems

---
layout: center
---

<Tweet id="1912954344724406475" user="minchoi" />

---
layout: default
class: text-small
---

```
<Question>
Where is this location in the image?
[Image: Street view of a city intersection]
</Question>

<crop_tool>
I'll analyze specific parts of this image to identify the location.
[Crop: Street sign visible in upper right]
</crop_tool>

<reasoning>
I can see "8th Ave" on the green street sign. There appears to be text about a "safety walkway." I should also examine other identifying features. The building architecture might provide additional clues.
[Crop: Building with distinctive vertical panels]
The building appears to have a modern design with vertical panels. This could be "220 West 58th" based on the architectural style.
</reasoning>

<web_search_tool>
Searching: "8th Ave intersection near West 58th Street New York"
</web_search_tool>

<reasoning>
The search results confirm this is likely in Manhattan, New York City. The intersection of 8th Avenue and West 58th Street is in Midtown, near Columbus Circle. The architectural style of the building matches buildings in this area. I can now provide a specific answer with high confidence.
</reasoning>

<final_answer>
This location is the intersection of 8th Avenue and West 58th Street in Midtown Manhattan, New York City (coordinates approximately 40.7664° N, 73.9815° W). The image shows the northwest corner of this intersection, looking north up 8th Avenue.
</final_answer>
```

---
layout: default
---

# Background: Enterprise Applications

## The Shift to Agentic Document Synthesis

- Companies are trying to automate **highly skilled jobs** that require visual human judgement
- **Example**: Geospatial image analysis in defense 
  - Traditionally labor-intensive and expertise-dependent
- Agentic reasoning systems enable workflow automation involving visual judgement
- Requires robust, verifiable reasoning capabilities

---
layout: center
---

<iframe width="760" height="515" src="https://www.youtube.com/embed/dHc6tDcE8wk?start=45" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---
layout: default
---

# Background: Our Research Focus

## Self-Play Reinforcement Learning for Reasoning

- We focus on **"self-play" reinforcement learning (RL)** techniques
- Goal: Enhance multimodal reasoning with **minimal human annotation**
- Our approach:
  - Automated "semi-formal" verification methods
  - Creates a scalable path to improve multimodal reasoning

- The key innovation: Using **Generative Agents** to verify reasoning chains

---
layout: default
---

# Core Mechanism 1: Monte Carlo Rollouts

```mermaid
graph TD
    A[VLM Query + Image] --> P[Perception]
    P --> R[Reasoning]
    R --> F[Final Output]
    
    style A fill:#6272a4,stroke:#333,stroke-width:2px,color:black
    style P fill:#8be9fd,stroke:#333,stroke-width:2px,color:black
    style R fill:#50fa7b,stroke:#333,stroke-width:2px,color:black
    style F fill:#ffb86c,stroke:#333,stroke-width:2px,color:black
```

---
layout: default
---

# Core Mechanism 2: Train a Process Reward Model with Agentic Verification of Reasoning Traces

```mermaid
graph LR
    Img[Image] --> VA
    Query[Query] --> VA
    RTrace[Reasoning Trace] --> VA
    
    VA[Verification Agent] -- Uses --> Tools[Tools Access]
    VA -- Evaluates --> Judgment[Valid: Yes/No]
    
    Judgment --> TrainData
    RTrace -.-> TrainData
    TrainData[Training Data<br>Trace + Judgment] --> PRM[Process Reward Model]
    
    style Img fill:#def,stroke:#333,color:black
    style Query fill:#def,stroke:#333,color:black
    style RTrace fill:#bfa,stroke:#333,color:black
    style VA fill:#f9f,stroke:#333,color:black
    style Judgment fill:#ff9,stroke:#333,color:black
    style PRM fill:#9cf,stroke:#333,color:black
    style TrainData fill:white,stroke:#333, color:black
    style Tools fill:white,stroke:#333,color:black
```

---
layout: default
---

# 5. RLHF for Fine-tuning the VLM

## Closing the Loop:

1.  **VLM Generates Reasoning**: The primary VLM generates a sequence of reasoning steps (including perception actions) to answer a query about an image.
    ```
    VLM: [Step 1] -> [Step 2] -> ... -> [Final Answer]
    ```