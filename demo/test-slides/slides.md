---
# You can also start simply with 'default'
theme: seriph
# random image from a curated Unsplash collection by Anthony
# like them? see https://unsplash.com/collections/94734566/slidev
background: https://cover.sli.dev
# some information about your slides (markdown enabled)
title: "Multimodal Reasoning & RLHF: Training a Process Reward Model"
info: |
  A presentation on training a Process Reward Model using an agentic approach
  to verify the validity of reasoning generated by a Vision Language Model.

  Learn more at [Sli.dev](https://sli.dev)
# apply unocss classes to the current slide
class: text-center
# https://sli.dev/features/drawing
drawings:
  persist: false
# slide transition: https://sli.dev/guide/animations.html#slide-transitions
transition: slide-left
# enable MDC Syntax: https://sli.dev/features/mdc
mdc: true
# open graph
# seoMeta:
#  ogImage: https://cover.sli.dev
# add scripts for Twitter widget
headersFrontmatter:
  scripts:
    - src: https://platform.twitter.com/widgets.js
      async: true
---

# Multimodal Reasoning & RL

### "Age of Experience", Test-time Scaling, Self-Improving Agents, PRMs

<div class="abs-br m-6 text-xl">
  <button @click="$slidev.nav.openInEditor()" title="Open in Editor" class="slidev-icon-btn">
    <carbon:edit />
  </button>
  <!-- <a href="https://github.com/slidevjs/slidev" target="_blank" class="slidev-icon-btn">
    <carbon:logo-github />
  </a> -->
</div>

<!--
The last comment block of each slide will be treated as slide notes. It will be visible and editable in Presenter Mode along with the slide. [Read more in the docs](https://sli.dev/guide/syntax.html#notes)
-->

---
layout: default
---

# Outline

1.  **Background: The Rise of Multimodal Reasoning**
    - Frontier AI products (Operator, Manus)
    - Enterprise Applications
2.  **The Age of Experience: Self-Improving Agents with RL and PRMs**
    - Improving agents with experience without human annotation
    - Agentic Verification
3.  **Core Mechanism 1: Monte Carlo Rollouts**
    - Generating experiences
4.  **Core Mechanism 2: Generating Rewards for Reasoning Trace**
    - Verifying reasoning traces using specialized agents with tools
5.  **RL Policy Model, Inference with PRM**
    - Use the newly trained PRM to improve model inference at test time

---
layout: default
---

# Background: The Rise of Multimodal Reasoning

## Industry Emergence

- **Multimodal reasoning** powers cutting-edge products like:
  - OpenAI Operator
  - Manus
  - ChatGPT's o3 agentic capabilities
- These systems can zoom into images, search the web, and use code interpreters to answer complex multimodal queries
- Represents a significant leap beyond unimodal AI systems

---
layout: center
---

<Tweet id="1912954344724406475" user="minchoi" />

---
layout: default
class: text-small
---

```
<Question>
Where is this location in the image?
[Image: Street view of a city intersection]
</Question>

<crop_tool>
I'll analyze specific parts of this image to identify the location.
[Crop: Street sign visible in upper right]
</crop_tool>

<reasoning>
I can see "8th Ave" on the green street sign. There appears to be text about a "safety walkway." I should also examine other identifying features. The building architecture might provide additional clues.
[Crop: Building with distinctive vertical panels]
The building appears to have a modern design with vertical panels. This could be "220 West 58th" based on the architectural style.
</reasoning>

<web_search_tool>
Searching: "8th Ave intersection near West 58th Street New York"
</web_search_tool>

<reasoning>
The search results confirm this is likely in Manhattan, New York City. The intersection of 8th Avenue and West 58th Street is in Midtown, near Columbus Circle. The architectural style of the building matches buildings in this area. I can now provide a specific answer with high confidence.
</reasoning>

<final_answer>
This location is the intersection of 8th Avenue and West 58th Street in Midtown Manhattan, New York City (coordinates approximately 40.7664° N, 73.9815° W). The image shows the northwest corner of this intersection, looking north up 8th Avenue.
</final_answer>
```

---
layout: default
---

# Background: Enterprise Applications

## The Shift to Agentic Document Synthesis

- Companies are trying to automate **highly skilled jobs** that require visual human judgement
- **Example**: Geospatial image analysis in defense 
  - Traditionally labor-intensive and expertise-dependent
- Agentic reasoning systems enable workflow automation involving visual judgement
- Requires robust, verifiable reasoning capabilities

---
layout: center
---

<iframe width="760" height="515" src="https://www.youtube.com/embed/dHc6tDcE8wk?start=45" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

---
layout: default
---

# Background: Our Research Focus

## Self-Play Reinforcement Learning for Reasoning

- We focus on **"self-play" reinforcement learning (RL)** techniques
- Goal: Enhance multimodal reasoning with **minimal human annotation**
- Our approach:
  - Automated "semi-formal" verification methods
  - Creates a scalable path to improve multimodal reasoning

- The key innovation: Using **Generative Agents** to verify reasoning chains

---
layout: default
---

# Core Mechanism 1: Monte Carlo Rollouts

```mermaid
graph TD
    A[VLM Query + Image] --> P[Perception]
    P --> R[Reasoning]
    R --> F[Final Output]
    
    style A fill:#6272a4,stroke:#333,stroke-width:2px,color:black
    style P fill:#8be9fd,stroke:#333,stroke-width:2px,color:black
    style R fill:#50fa7b,stroke:#333,stroke-width:2px,color:black
    style F fill:#ffb86c,stroke:#333,stroke-width:2px,color:black
```

---
layout: default
---

# Core Mechanism 2: Use Generative Agent to Generate Rewards for Perception and Reasoning Trace

```mermaid
graph TD
    subgraph Inputs
        Img[Image]
        Query
    end

    subgraph "VLM (Agent) Processing"
        direction LR
        VLM_Agent["VLM (Agent)"]
        VLM_Agent -- Analyzes --> Img
        VLM_Agent -- Understands --> Query
        
        VLM_Agent -- Generates --> RStep1["Reasoning Step 1 (e.g., focus area)"]
        RStep1 --> ToolA_Call{Use Tool A}
        ToolA_Call --> ToolA_Out["Tool A Output"]
        ToolA_Out --> RStep2["Reasoning Step 2 (e.g., extract attribute)"]
        RStep2 --> ToolB_Call{Use Tool B}
        ToolB_Call --> ToolB_Out["Tool B Output"]
        ToolB_Out --> RStepN["..."]
        RStepN --> FinalTrace["Complete Visual Reasoning Trace"]
    end

    subgraph "Rating Process"
        Rater
        FinalTrace --> Rater
        Rater -- Assigns --> RewardScore["Reward Score"]
    end

    subgraph "PRM Training"
        TrainingData["(Trace, Score) Pairs"]
        RewardScore --> TrainingData
        FinalTrace -.-> TrainingData
        TrainingData -- Trains --> VPRM[Vision Process Reward Model]
    end

    Inputs --> VLM_Agent
    VLM_Agent --> FinalTrace

    style Img fill:#def,stroke:#333,stroke-width:2px
    style Query fill:#def,stroke:#333,stroke-width:2px
    style VLM_Agent fill:#f9f,stroke:#333,stroke-width:2px
    style Rater fill:#ff9,stroke:#333,stroke-width:2px
    style VPRM fill:#9cf,stroke:#333,stroke-width:2px
    style FinalTrace fill:#bfa,stroke:#333,stroke-width:2px
```
**Figure 2: Generating Reward Scores for Vision Process Reward Model Training (Illustrative)**

## How we generate reward scores for the V-PRM:

-   **Focus**: Specifically on the visual perception and grounding aspects of the VLM's reasoning.
    -   *Did the VLM correctly identify relevant objects?*
    -   *Did it accurately interpret spatial relationships pertinent to the query?*
    -   *Were the visual features attended to appropriate for the reasoning step?*
-   **Human-in-the-loop / Oracle**:
    -   Initially, human annotators (or a more powerful oracle model) provide scores for individual perception-action steps generated by the VLM.
    -   Example: Given an image and a query like "What is the red object to the left of the blue cube?", a reasoning step might involve:
        1.  `Agent`: "I see a red sphere." (Focus on red sphere) -> `Human/Oracle Score`: +1 (correct identification)
        2.  `Agent`: "It is to the left of a blue cube." (Focus on blue cube and relative position) -> `Human/Oracle Score`: +1 (correct spatial reasoning)
-   These scored (perception step, reward) pairs become the training data for the **Vision Process Reward Model (V-PRM)**.
-   The V-PRM learns to predict these reward scores for unseen perception steps.

---
layout: default
---

# 5. RLHF for Fine-tuning the VLM

## Closing the Loop:

1.  **VLM Generates Reasoning**: The primary VLM generates a sequence of reasoning steps (including perception actions) to answer a query about an image.
    ```
    VLM: [Step 1] -> [Step 2] -> ... -> [Final Answer]
    ```